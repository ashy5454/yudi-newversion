# üé§ Voice Output System - Complete Summary

## What You're Actually Using

Your codebase uses **TWO different voice output systems** depending on the use case:

---

## 1. üéØ **Indic-TTS (Coqui TTS with XTTS v2)** - Primary Backend TTS

### Where It's Used:
- **Backend endpoint:** `/api/gemini-text-to-indic-tts`
- **Backend endpoint:** `/api/tts`
- **Location:** `backend/services/tts_indic.py`

### Technology:
- **Engine:** Coqui TTS (AI4Bharat Indic-TTS)
- **Model:** XTTS v2 (`tts_models/multilingual/multi-dataset/xtts_v2`)
- **Languages:** Hindi, Telugu, English
- **Voices:** Male/Female options
- **Format:** WAV audio files
- **Sample Rate:** 22050 Hz

### How It Works:
1. Text response is generated by Gemini (with Unified Brain)
2. Text is passed to Indic-TTS engine
3. Audio is synthesized using XTTS v2 model
4. WAV bytes are returned

### Code Location:
```python
# backend/main.py:862
audio_bytes, sample_rate = tts_engine.synthesize(
    text=gemini_response_text,
    language=language_code,
    speaker=speaker
)
```

---

## 2. üîä **Gemini Live API (Native Audio)** - Frontend Real-Time Voice

### Where It's Used:
- **Frontend:** Voice chat interface (`src/components/audio/`)
- **Hook:** `src/hooks/media/use-live-api.ts`
- **Component:** `src/components/audio/console/ControlTray.tsx`

### Technology:
- **Engine:** Google Gemini Live API (native audio streaming)
- **Format:** Real-time PCM16 audio stream
- **Connection:** WebSocket-based bidirectional audio
- **Languages:** Supports multiple languages (configured per persona)
- **Voices:** Prebuilt voice configurations from Gemini

### How It Works:
1. Frontend connects to Gemini Live API via WebSocket
2. User speaks ‚Üí Audio sent to Gemini in real-time
3. Gemini processes and responds with audio stream
4. Audio is played directly in browser (no TTS conversion needed)

### Code Location:
```typescript
// src/hooks/media/use-live-api.ts
// Uses @google/genai LiveAPI client
// Audio streams directly from Gemini
```

---

## üìä Summary Table

| System | Location | Use Case | Technology | Format |
|--------|----------|----------|------------|--------|
| **Indic-TTS** | Backend | Text ‚Üí Audio files | Coqui TTS (XTTS v2) | WAV files |
| **Gemini Live API** | Frontend | Real-time voice chat | Google Gemini Native Audio | PCM16 stream |

---

## üéØ Which One Is Actually Used?

### For Backend Voice Generation:
‚úÖ **Indic-TTS (XTTS v2)** - This is what `/api/gemini-text-to-indic-tts` uses

### For Frontend Voice Chat:
‚úÖ **Gemini Live API** - This is what the voice chat UI uses

### "Smart Voice Router":
‚ö†Ô∏è **Exists but NOT fully implemented**
- The router exists in `backend/services/voice_router_service.py`
- It can route to Indic-TTS OR Gemini Native
- BUT: Gemini Native Audio routing is a **placeholder** (not actually implemented server-side)
- The main endpoint `/api/gemini-text-to-indic-tts` **always uses Indic-TTS**

---

## üîç Current Implementation Status

### ‚úÖ Fully Working:
1. **Indic-TTS** - Complete implementation, working
2. **Gemini Live API** - Complete frontend implementation, working

### ‚ö†Ô∏è Partially Implemented:
1. **Smart Voice Router** - Routing logic exists, but Gemini Native Audio is placeholder

### ‚ùå Not Implemented:
1. **Backend Gemini Native Audio** - Server-side Gemini audio generation is not implemented (router just returns a message to use client-side)

---

## üí° Bottom Line

**For voice output, you're using:**
1. **Indic-TTS (XTTS v2)** - For backend-generated audio files
2. **Gemini Live API** - For frontend real-time voice chat

Both systems are working and serve different purposes!

